{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wrote this notebook after reading this very instructional article: https://jacobbuckman.com/post/tensorflow-the-confusing-parts-1/\n",
    "\n",
    "Most of the code here is taken from this great original article, comments are mine.\n",
    "\n",
    "The main abstractions of Tensorflow are clearly presented, with constant reference to the computation graph which result in a good understanding of what happens under the hood for each instruction.\n",
    "\n",
    "This notebook summarises the main points of the article and may be used for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/profiling/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The computation graph\n",
    "\n",
    "The computation graph can be seen as a series of instructions to perform computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "two_node = tf.constant(2)\n",
    "print(two_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everytime `tf.constant`is called, it adds a new node to the graph. The following instructions will create two distinct nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_node = tf.constant(2)\n",
    "two_node = tf.constant(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, assigning a new variable to an existing node just copies the pointer to that node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_node = tf.constant(2)  # create the node\n",
    "another_pointer_at_two_node = two_node  # no node added to the graph, just copied the pointer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Tensor(\"Const_4:0\", shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "two_node = tf.constant(2)\n",
    "another_pointer_at_two_node = two_node\n",
    "two_node = None\n",
    "print(two_node)\n",
    "print(another_pointer_at_two_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `+` operator is overloaded in Tensorflow, so that it actually performs tensor addition (similar to using `tf.add(tensor_1, tensor_2)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_node = tf.constant(2)\n",
    "three_node = tf.constant(3)\n",
    "sum_node = two_node + three_node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The session\n",
    "\n",
    "The session handles memory allocation and optimization to actually perform the operations described by the computation graph.\n",
    "\n",
    "A graph and a session are the mandaroty elements to perform computations in Tensorflow.\n",
    "\n",
    "The session contains a pointer to the graph which contains pointers to all the nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "two_node = tf.constant(2)\n",
    "three_node = tf.constant(3)\n",
    "sum_node = two_node + three_node\n",
    "sess = tf.Session()\n",
    "print(sess.run(sum_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several nodes can be evaluated at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5]\n"
     ]
    }
   ],
   "source": [
    "two_node = tf.constant(2)\n",
    "three_node = tf.constant(3)\n",
    "sum_node = two_node + three_node\n",
    "sess = tf.Session()\n",
    "print(sess.run([two_node, sum_node]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sess.run()` is usually a bottleneck in Tensorflow: whenever possible, it is better to return multiple values from a single call to `sess.run()`rather than calling it multiple times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Placeholders and feed_dict\n",
    "\n",
    "A placeholder node is a node that is meant to receive external input. The external input is provided using the `feed_dict` argument in `sess.run()``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "input_placeholder = tf.placeholder(tf.int32)\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(input_placeholder, feed_dict={input_placeholder: 2}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables\n",
    "\n",
    "Variables are usually used to store model parameters: the variables change values during the training phase and then are held constant during the prediction phase. `tf.get_variable(name, shape)` is used to create a new variable. `name` should be a unique variable name for the while graph, `shape` sets the size of the tensor, `[]` corresponding to a scalar.\n",
    "\n",
    "A variable can only be evaluated *after* storing a value into it, using either initializers or `tf.assign()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `tf.assign()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "count_variable = tf.get_variable(\"count\", [])\n",
    "zero_node = tf.constant(0.)\n",
    "assign_node = tf.assign(count_variable, zero_node)\n",
    "sess = tf.Session()\n",
    "sess.run(assign_node)\n",
    "print(sess.run(count_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.assign(node, value)` returns `value`\n",
    "\n",
    "`tf.assign()` has a side effect: when computation runs through the `assign_node` node, it changes the value of `count_variable` to `zero_node`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(assign_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "const_init_node = tf.constant_initializer(0.)\n",
    "count_variable = tf.get_variable(\"count2\", [], initializer=const_init_node)\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "print(sess.run(count_variable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.global_variables_initialier()` is a node with side effects: it goes through all the initializers and initialises the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "Example below uses a linear regression example, demonstraing the use of optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss 0.45\n",
      "Step 1, loss 0.47\n",
      "Step 2, loss 1.04\n",
      "Step 3, loss 1.05\n",
      "Step 4, loss 0.74\n",
      "Step 5, loss 1.34\n",
      "Step 6, loss 0.71\n",
      "Step 7, loss 1.32\n",
      "Step 8, loss 0.61\n",
      "Step 9, loss 1.02\n",
      "Step 10, loss 1.27\n",
      "Step 11, loss 0.95\n",
      "Step 12, loss 0.60\n",
      "Step 13, loss 0.85\n",
      "Step 14, loss 0.62\n",
      "Step 15, loss 0.65\n",
      "Step 16, loss 1.29\n",
      "Step 17, loss 0.67\n",
      "Step 18, loss 0.69\n",
      "Step 19, loss 1.04\n",
      "Step 20, loss 1.04\n",
      "Step 21, loss 1.13\n",
      "Step 22, loss 0.59\n",
      "Step 23, loss 0.87\n",
      "Step 24, loss 0.37\n",
      "Step 25, loss 0.42\n",
      "Step 26, loss 0.42\n",
      "Step 27, loss 0.55\n",
      "Step 28, loss 0.77\n",
      "Step 29, loss 0.74\n",
      "Step 30, loss 0.74\n",
      "Step 31, loss 0.63\n",
      "Step 32, loss 0.39\n",
      "Step 33, loss 1.14\n",
      "Step 34, loss 0.73\n",
      "Step 35, loss 1.19\n",
      "Step 36, loss 0.41\n",
      "Step 37, loss 0.95\n",
      "Step 38, loss 1.15\n",
      "Step 39, loss 0.70\n",
      "Step 40, loss 0.88\n",
      "Step 41, loss 0.73\n",
      "Step 42, loss 0.62\n",
      "Step 43, loss 0.94\n",
      "Step 44, loss 0.34\n",
      "Step 45, loss 0.74\n",
      "Step 46, loss 0.40\n",
      "Step 47, loss 1.06\n",
      "Step 48, loss 1.10\n",
      "Step 49, loss 0.84\n",
      "Step 50, loss 1.08\n",
      "Step 51, loss 0.37\n",
      "Step 52, loss 0.83\n",
      "Step 53, loss 0.55\n",
      "Step 54, loss 0.96\n",
      "Step 55, loss 0.61\n",
      "Step 56, loss 0.73\n",
      "Step 57, loss 0.98\n",
      "Step 58, loss 0.44\n",
      "Step 59, loss 0.72\n",
      "Step 60, loss 0.95\n",
      "Step 61, loss 0.62\n",
      "Step 62, loss 0.47\n",
      "Step 63, loss 0.90\n",
      "Step 64, loss 0.65\n",
      "Step 65, loss 0.38\n",
      "Step 66, loss 0.65\n",
      "Step 67, loss 0.56\n",
      "Step 68, loss 0.47\n",
      "Step 69, loss 0.91\n",
      "Step 70, loss 0.86\n",
      "Step 71, loss 0.47\n",
      "Step 72, loss 0.88\n",
      "Step 73, loss 0.54\n",
      "Step 74, loss 0.68\n",
      "Step 75, loss 0.48\n",
      "Step 76, loss 0.47\n",
      "Step 77, loss 0.31\n",
      "Step 78, loss 0.59\n",
      "Step 79, loss 0.29\n",
      "Step 80, loss 0.59\n",
      "Step 81, loss 0.93\n",
      "Step 82, loss 0.64\n",
      "Step 83, loss 0.82\n",
      "Step 84, loss 0.89\n",
      "Step 85, loss 0.88\n",
      "Step 86, loss 0.37\n",
      "Step 87, loss 0.60\n",
      "Step 88, loss 0.67\n",
      "Step 89, loss 0.64\n",
      "Step 90, loss 0.29\n",
      "Step 91, loss 0.34\n",
      "Step 92, loss 0.75\n",
      "Step 93, loss 0.51\n",
      "Step 94, loss 0.41\n",
      "Step 95, loss 0.59\n",
      "Step 96, loss 0.42\n",
      "Step 97, loss 0.84\n",
      "Step 98, loss 0.31\n",
      "Step 99, loss 0.41\n",
      "True parameters:     m=0.5404, b=0.6543\n",
      "Learned parameters:  m=0.0966, b=0.1660\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "### build the graph\n",
    "## first set up the model parameters\n",
    "m = tf.get_variable(\"m\", [], initializer=tf.constant_initializer(0.))\n",
    "b = tf.get_variable(\"b\", [], initializer=tf.constant_initializer(0.))\n",
    "init = tf.global_variables_initializer()  # add the initialization node to the graph\n",
    "\n",
    "## then set up the computations\n",
    "input_placeholder = tf.placeholder(tf.float32)\n",
    "output_placeholder = tf.placeholder(tf.float32)\n",
    "\n",
    "x = input_placeholder\n",
    "y = output_placeholder\n",
    "y_guess = m * x + b\n",
    "\n",
    "loss = tf.square(y - y_guess)\n",
    "\n",
    "## finally, set up the optimizer and minimization node\n",
    "optimizer = tf.train.GradientDescentOptimizer(1e-3)  # this is NOT a node, just a python object\n",
    "train_op = optimizer.minimize(loss)  # this is a node in the graph\n",
    "\n",
    "### start the session\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "### perform the training loop\n",
    "\n",
    "\n",
    "## set up problem\n",
    "true_m = random.random()\n",
    "true_b = random.random()\n",
    "\n",
    "for update_i in range(100):\n",
    "  ## (1) get the input and output\n",
    "  input_data = random.random()\n",
    "  output_data = true_m * input_data + true_b\n",
    "\n",
    "  ## (2), (3), and (4) all take place within a single call to sess.run()!\n",
    "  _loss, _ = sess.run([loss, train_op], feed_dict={input_placeholder: input_data, output_placeholder: output_data})\n",
    "  print('Step %d, loss %.2f' % (update_i, _loss))\n",
    "\n",
    "### finally, print out the values we learned for our two variables\n",
    "print(\"True parameters:     m=%.4f, b=%.4f\" % (true_m, true_b))\n",
    "print(\"Learned parameters:  m=%.4f, b=%.4f\" % tuple(sess.run([m, b])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes on the code above:\n",
    "* `optimizer` points to a Python object, it is not a node added to the graph\n",
    "* `train_op = ...` on the other hand adds a node added to the graph, and has side effects when evaluated: it performs one step of gradient descend, by evaluating the gradient of the loss function wrt `m` and `b` and updating the `m = m - alpha * dJ/dm` and `b = b - alpha * dJ / db`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging with `tf.Print()`\n",
    "\n",
    "`tf.Print(node_to_copy, [nodes to print])` adds a node to the graph which has both an output and side effect. `tf.Print()` will create a copy of the `node_to_copy` and as a side effect prints the values of the nodes in nodes to print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "two_node = tf.constant(2)\n",
    "three_node = tf.constant(3)\n",
    "sum_node = two_node + three_node\n",
    "print_sum_node = tf.Print(sum_node, [two_node, three_node])\n",
    "sess = tf.Session()\n",
    "print(sess.run(print_sum_node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.Print()` adds a node to the graph. For the side effect to occur, the node has to be on the computation path (otherwise nothing happens ;)).\n",
    "\n",
    "Additional resources for debugging: https://wookayin.github.io/tensorflow-talk-debugging/#1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
